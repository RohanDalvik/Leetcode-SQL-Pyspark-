{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e916cb-b3ee-4a91-97f9-fec773c34172",
     "showTitle": true,
     "title": "Recycable & Low Fact Product"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n|product_id|\n+----------+\n|         1|\n|         3|\n+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"low_fats\", StringType(), nullable=False),\n",
    "    StructField(\"recyclable\", StringType(), nullable=False)\n",
    "])\n",
    "\n",
    "\n",
    "data = [\n",
    "    (0, 'Y', 'N'),\n",
    "    (1, 'Y', 'Y'),\n",
    "    (2, 'N', 'Y'),\n",
    "    (3, 'Y', 'Y'),\n",
    "    (4,'N','N')\n",
    "]\n",
    "\n",
    "# Create the DataFrame\n",
    "df = spark.createDataFrame(data, schema=schema)\n",
    "filtered_df = df.filter((df.low_fats == 'Y') & (df.recyclable == 'Y'))\n",
    "result_df = filtered_df.select(\"product_id\")\n",
    "# Show the DataFrame\n",
    "result_df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03ab9754-88f0-4191-879c-244707f1bce9",
     "showTitle": true,
     "title": "1378. Replace Employee ID With The Unique Identifier"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n|unique_id|     name|\n+---------+---------+\n|     null|    Alice|\n|     null|      Bob|\n|        2|     Meir|\n|        3|  Winston|\n|        1|Jonanthan|\n+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "\n",
    "schema1 = StructType([StructField(\"id\",IntegerType(), nullable = False),\n",
    "         StructField(\"name\", StringType(), nullable = False)\n",
    "\n",
    "         ])\n",
    "\n",
    "data1 = [\n",
    "    (1,\"Alice\"),\n",
    "    (7,\"Bob\"),\n",
    "    (11,\"Meir\"),\n",
    "    (90,\"Winston\"),\n",
    "    (3,\"Jonanthan\")\n",
    "]\n",
    "employees_df = spark.createDataFrame(data = data1, schema=schema1)\n",
    "schema2 = StructType([StructField(\"id\",IntegerType(),nullable = False),\n",
    "         StructField(\"unique_id\",IntegerType(),nullable = False)\n",
    "])\n",
    "\n",
    "data2 = [\n",
    "    (3,1),\n",
    "    (11,2),\n",
    "    (90,3)\n",
    "   \n",
    "]\n",
    "employee_uni_df = spark.createDataFrame(data =data2, schema=schema2)\n",
    "\n",
    "result_df = employees_df.join(broadcast(employee_uni_df), on=\"id\", how=\"left\") \\\n",
    "    .select(\"unique_id\", \"name\")\n",
    "\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "#select \n",
    "#eu.unique_id as unique_id, e.name as name\n",
    "#from Employees e left join EmployeeUNI eu on e.id = eu.id\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c80b0c1-3f50-47db-9161-829db28771b0",
     "showTitle": true,
     "title": "627. Swap Salary"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+------+\n| id|name|sex|salary|\n+---+----+---+------+\n|  1|   A|  f|  2500|\n|  2|   B|  m|  1500|\n|  3|   C|  f|  5500|\n|  4|   D|  m|   500|\n+---+----+---+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = [\n",
    "    (1, \"A\", \"m\", 2500),\n",
    "    (2, \"B\", \"f\", 1500),\n",
    "    (3, \"C\", \"m\", 5500),\n",
    "    (4, \"D\", \"f\", 500)\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"sex\", \"salary\"])\n",
    "\n",
    "# Swap 'f' and 'm' values in the 'sex' column\n",
    "result_df = df.withColumn(\"sex\", when(df.sex == \"f\", \"m\").otherwise(\"f\"))\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8773128e-a1e3-450d-9f54-8e92d1f95298",
     "showTitle": true,
     "title": "1068. Product Sales Analysis I"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+-----+\n|product_name|year|price|\n+------------+----+-----+\n|       Nokia|2008| 5000|\n|       Nokia|2009| 5000|\n|       Apple|2011| 9000|\n+------------+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sales_data = [\n",
    "    (1, 100, 2008, 10, 5000),\n",
    "    (2, 100, 2009, 12, 5000),\n",
    "    (7, 200, 2011, 15, 9000)\n",
    "]\n",
    "\n",
    "product_data = [\n",
    "    (100, \"Nokia\"),\n",
    "    (200, \"Apple\"),\n",
    "    (300, \"Samsung\")\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "sales_df = spark.createDataFrame(sales_data, [\"sale_id\", \"product_id\", \"year\", \"quantity\", \"price\"])\n",
    "product_df = spark.createDataFrame(product_data, [\"product_id\", \"product_name\"])\n",
    "\n",
    "# Perform SQL join to get the desired result\n",
    "result_df = sales_df.join(product_df, on=\"product_id\", how=\"inner\") \\\n",
    "    .select(\"product_name\", \"year\", \"price\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n",
    "\n",
    "#SELECT product_name ,year,price from Sales JOIN Product on Sales.product_id   = Product.product_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62136e1f-d4b3-45bf-9ddb-b15c162b993a",
     "showTitle": true,
     "title": "1179. Reformat Department Table"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n| id|Jan_Revenue|Feb_Revenue|Mar_Revenue|Apr_Revenue|May_Revenue|Jun_Revenue|Jul_Revenue|Aug_Revenue|Sep_Revenue|Oct_Revenue|Nov_Revenue|Dec_Revenue|\n+---+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n|  1|       8000|       7000|       6000|       null|       null|       null|       null|       null|       null|       null|       null|       null|\n|  3|       null|      10000|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|\n|  2|       9000|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|       null|\n+---+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "\n",
    "data = [\n",
    "    (1, 8000, \"Jan\"),\n",
    "    (2, 9000, \"Jan\"),\n",
    "    (3, 10000, \"Feb\"),\n",
    "    (1, 7000, \"Feb\"),\n",
    "    (1, 6000, \"Mar\")\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"id\", \"revenue\", \"month\"])\n",
    "\n",
    "# Pivot the data\n",
    "pivoted_df = df.groupBy(\"id\").pivot(\"month\", [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]).sum(\"revenue\")\n",
    "\n",
    "# Rename the columns\n",
    "columns = [\"id\"] + [f\"{month}_Revenue\" for month in [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]]\n",
    "result_df = pivoted_df.toDF(*columns)\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n",
    "\n",
    "#SELECT id,\n",
    "#      SUM(if(month = 'Jan', revenue, null)) AS Jan_Revenue,\n",
    "#      SUM(if(month = 'Feb', revenue, null)) AS Feb_Revenue,\n",
    "#       SUM(if(month = 'Mar', revenue, null)) AS Mar_Revenue,\n",
    "#      SUM(if(month = 'Apr', revenue, null)) AS Apr_Revenue,\n",
    "#      SUM(if(month = 'May', revenue, null)) AS May_Revenue,\n",
    "#      SUM(if(month = 'Jun', revenue, null)) AS Jun_Revenue,\n",
    "#      SUM(if(month = 'Jul', revenue, null)) AS Jul_Revenue,\n",
    "#      SUM(if(month = 'Aug', revenue, null)) AS Aug_Revenue,\n",
    "#      SUM(if(month = 'Sep', revenue, null)) AS Sep_Revenue,\n",
    "#      SUM(if(month = 'Oct', revenue, null)) AS Oct_Revenue,\n",
    "#       SUM(if(month = 'Nov', revenue, null)) AS Nov_Revenue,\n",
    "#      SUM(if(month = 'Dec', revenue, null)) AS Dec_Revenue\n",
    "#FROM Department\n",
    "#GROUP BY id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86ad694e-c6e4-46f5-adde-91a3c94f001f",
     "showTitle": true,
     "title": "1484. Group Sold Products By The Date"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+----------------------------+\n|sell_date |num_sold|products                    |\n+----------+--------+----------------------------+\n|2020-05-30|3       |Basketball,Headphone,T-Shirt|\n|2020-06-01|2       |Bible,Pencil                |\n|2020-06-02|2       |Mask,Mask                   |\n+----------+--------+----------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, count, concat_ws , sort_array\n",
    "\n",
    "\n",
    "data = [\n",
    "    (\"2020-05-30\", \"Headphone\"),\n",
    "    (\"2020-06-01\", \"Pencil\"),\n",
    "    (\"2020-06-02\", \"Mask\"),\n",
    "    (\"2020-05-30\", \"Basketball\"),\n",
    "    (\"2020-06-01\", \"Bible\"),\n",
    "    (\"2020-06-02\", \"Mask\"),\n",
    "    (\"2020-05-30\", \"T-Shirt\")\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data, [\"sell_date\", \"product\"])\n",
    "\n",
    "# Group and aggregate the data\n",
    "result_df = df.groupBy(\"sell_date\").agg(count(\"product\").alias(\"num_sold\"),\n",
    "               concat_ws(\",\", sort_array(collect_list(\"product\"))).alias(\"products\")).orderBy(\"sell_date\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "#SELECT \n",
    "#    sell_date,\n",
    "#    COUNT(DISTINCT product) AS num_sold,\n",
    "#    GROUP_CONCAT(DISTINCT product ORDER BY product) AS products\n",
    "#FROM\n",
    "#    Activities\n",
    "#GROUP BY\n",
    "#    sell_date\n",
    "#ORDER BY\n",
    "#    sell_date;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "482695b7-ea11-46b4-b3f0-5a84b5e62072",
     "showTitle": true,
     "title": "175. Combine Two Tables"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-------------+--------+\n|firstName|lastName|city         |state   |\n+---------+--------+-------------+--------+\n|Allen    |Wang    |null         |null    |\n|Bob      |Alice   |New York City|New York|\n+---------+--------+-------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "person_data = [\n",
    "    (1, \"Wang\", \"Allen\"),\n",
    "    (2, \"Alice\", \"Bob\")\n",
    "]\n",
    "\n",
    "address_data = [\n",
    "    (1, 2, \"New York City\", \"New York\"),\n",
    "    (2, 3, \"Leetcode\", \"California\")\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "person_df = spark.createDataFrame(person_data, [\"personId\", \"lastName\", \"firstName\"])\n",
    "address_df = spark.createDataFrame(address_data, [\"addressId\", \"personId\", \"city\", \"state\"])\n",
    "\n",
    "# Perform left join and select the required columns\n",
    "result_df = person_df.join(address_df, on=\"personId\", how=\"left\").select(\"firstName\", \"lastName\", \"city\", \"state\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show(truncate=False)\n",
    "\n",
    "#select FirstName, LastName, City, State\n",
    "#from Person left join Address\n",
    "#on Person.PersonId = Address.PersonId;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb51ea9a-6727-4707-b159-f0d88d771cd1",
     "showTitle": true,
     "title": "577. Employee Bonus"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n|name|bonus|\n+----+-----+\n|Brad| null|\n|John| null|\n| Dan|  500|\n+----+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "employee_data = [\n",
    "    (3, \"Brad\", None, 4000),\n",
    "    (1, \"John\", 3, 1000),\n",
    "    (2, \"Dan\", 3, 2000),\n",
    "    (4, \"Thomas\", 3, 4000)\n",
    "]\n",
    "\n",
    "bonus_data = [\n",
    "    (2, 500),\n",
    "    (4, 2000)\n",
    "]\n",
    "\n",
    "# Create DataFrames\n",
    "employee_df = spark.createDataFrame(employee_data, [\"empId\", \"name\", \"supervisor\", \"salary\"])\n",
    "bonus_df = spark.createDataFrame(bonus_data, [\"empId\", \"bonus\"])\n",
    "\n",
    "# Perform left join and filter rows\n",
    "result_df = employee_df.join(bonus_df, on=\"empId\", how=\"left_outer\").filter(\"bonus < 1000 or bonus is Null\").select(\"name\", \"bonus\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n",
    "\n",
    "#SELECT e1.name,b1.bonus from Employee e1 LEFT JOIN Bonus b1 ON e1.empId = b1.empId \n",
    "#WHERE b1.bonus < 1000 or b1.empId is Null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9086d4cd-fbd3-443e-b36d-684ebc17104e",
     "showTitle": true,
     "title": "620. Not Boring Movies"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------+------+\n| id|     movie|description|rating|\n+---+----------+-----------+------+\n|  5|House card|Interesting|   9.1|\n|  1|       War|   great 3D|   8.9|\n+---+----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "data = [\n",
    "    (1, \"War\", \"great 3D\", 8.9),\n",
    "    (2, \"Science\", \"fiction\", 8.5),\n",
    "    (3, \"irish\", \"boring\", 6.2),\n",
    "    (4, \"Ice song\", \"Fantacy\", 8.6),\n",
    "    (5, \"House card\", \"Interesting\", 9.1)\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "cinema_df = spark.createDataFrame(data, [\"id\", \"movie\", \"description\", \"rating\"])\n",
    "\n",
    "# Filter and order the data\n",
    "result_df = cinema_df.filter((cinema_df.id % 2 == 1) & (cinema_df.description != \"boring\")) \\\n",
    "    .orderBy(cinema_df.rating.desc())\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n",
    "\n",
    "#select * from Cinema where description <> \"Boring\" and id%2 <> 0 order by rating desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2625132e-ed0e-4995-a950-ccf065ffe098",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n|  email|\n+-------+\n|a@b.com|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "data = [\n",
    "    (1, \"a@b.com\"),\n",
    "    (2, \"c@d.com\"),\n",
    "    (3, \"a@b.com\")\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "person_df = spark.createDataFrame(data, [\"id\", \"email\"])\n",
    "\n",
    "# Group by email and count occurrences\n",
    "result_df = person_df.groupBy(\"email\").count() .filter(\"count > 1\").select(\"email\")\n",
    "\n",
    "# Show the result\n",
    "result_df.show()\n",
    "\n",
    "# select email from Person group by email  having count(email)>1\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Leetcode SQL Pyspark Solutions 1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
